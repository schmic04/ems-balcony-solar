"""
SMARD API Module.

This module provides functions for accessing and managing SMARD API data.
It supports both direct internet access and local data storage/reuse.

Author: Generated by schmic04
Date: 2025-09-14
"""

from __future__ import annotations

import json
import logging
from dataclasses import dataclass
from datetime import UTC, date, datetime, time
from pathlib import Path
from typing import Any, NamedTuple

import pandas as pd
import requests

_LOGGER = logging.getLogger(__name__)

# Default timeout for HTTP requests (seconds)
DEFAULT_TIMEOUT = 30

# Constants for data processing
MIN_SERIES_ENTRY_LENGTH = 2
PREVIEW_ENTRIES_COUNT = 5


@dataclass
class SmardConfig:
    """Configuration for SMARD API module."""

    enable_local: bool = False
    data_dir: Path = Path("smard_data")

    def __post_init__(self) -> None:
        """Ensure data directory exists."""
        if self.enable_local and not self.data_dir.exists():
            self.data_dir.mkdir(parents=True, exist_ok=True)
            _LOGGER.info("Created directory: %s", self.data_dir)


# Module-level configuration
_config = SmardConfig()


def set_config(*, enable_local: bool = False, data_dir: str = "smard_data") -> None:
    """
    Configure global settings for the module.

    Args:
        enable_local: True to use local data, False to use network data
        data_dir: Directory for local JSON files

    """
    global _config  # noqa: PLW0603
    _config = SmardConfig(enable_local=enable_local, data_dir=Path(data_dir))


class SmardDataParams(NamedTuple):
    """Parameters for SMARD data requests."""

    filter_param: str
    region: str
    filter_copy: str
    region_copy: str
    resolution: str


def save_timestamps_to_file(
    timestamps: list[int],
    filter_param: str,
    region: str,
    resolution: str,
) -> bool:
    """
    Save timestamps to a local JSON file.

    Args:
        timestamps: List of timestamps
        filter_param: Filter parameter
        region: Region
        resolution: Time resolution

    Returns:
        True on success, False on error

    """
    filename = (
        _config.data_dir / f"timestamps_{filter_param}_{region}_{resolution}.json"
    )
    data = {
        "filter": filter_param,
        "region": region,
        "resolution": resolution,
        "timestamps": timestamps,
        "saved_at": datetime.now(UTC).isoformat(),
    }

    try:
        with filename.open("w", encoding="utf-8") as f:
            json.dump(data, f, indent=2, ensure_ascii=False)
        _LOGGER.debug("Timestamps saved to: %s", filename)
    except OSError:
        _LOGGER.exception("Error saving timestamps")
        return False
    else:
        return True


def load_timestamps_from_file(
    filter_param: str,
    region: str,
    resolution: str,
) -> list[int]:
    """
    Load timestamps from a local JSON file.

    Args:
        filter_param: Filter parameter
        region: Region
        resolution: Time resolution

    Returns:
        List of timestamps or empty list on error

    """
    filename = (
        _config.data_dir / f"timestamps_{filter_param}_{region}_{resolution}.json"
    )

    try:
        with filename.open(encoding="utf-8") as f:
            data = json.load(f)
        _LOGGER.debug("Timestamps loaded from: %s", filename)
        _LOGGER.debug("Saved at: %s", data.get("saved_at", "Unknown"))
        return data.get("timestamps", [])
    except FileNotFoundError:
        _LOGGER.debug("Local file not found: %s", filename)
        return []
    except (OSError, json.JSONDecodeError):
        _LOGGER.exception("Error loading timestamps")
        return []


def fetch_smard_timestamps(
    filter_param: str,
    region: str,
    resolution: str,
) -> list[int]:
    """
    Fetch timestamps either from local storage or network.

    Args:
        filter_param: Filter parameter (e.g. "1223" for power generation)
        region: Region (e.g. "DE" for Germany)
        resolution: Time resolution (e.g. "hour", "day", "week", "month", "year")

    Returns:
        List of available timestamps

    """
    # Try local data first if enabled
    if _config.enable_local:
        _LOGGER.debug("Using local timestamp data")
        timestamps = load_timestamps_from_file(filter_param, region, resolution)
        if timestamps:
            _LOGGER.debug(
                "Successfully loaded! Number of timestamps: %d", len(timestamps)
            )
            return timestamps
        _LOGGER.debug("No local data found, loading from network...")

    # Load from network
    _LOGGER.debug("Loading timestamps from network")

    url = (
        f"https://www.smard.de/app/chart_data/{filter_param}/"
        f"{region}/index_{resolution}.json"
    )

    try:
        _LOGGER.debug("Loading data from: %s", url)

        response = requests.get(url, timeout=DEFAULT_TIMEOUT)
        response.raise_for_status()

        # Parse JSON data
        data = response.json()
        timestamps = data.get("timestamps", [])

        # Save to local file if local mode is enabled
        if _config.enable_local and timestamps:
            save_timestamps_to_file(timestamps, filter_param, region, resolution)

        _LOGGER.debug("Successfully loaded! Number of timestamps: %d", len(timestamps))
    except requests.RequestException:
        _LOGGER.exception("Error loading data")
        return []
    except json.JSONDecodeError:
        _LOGGER.exception("Error parsing JSON data")
        return []
    else:
        return timestamps


def save_data_to_file(
    data: dict[str, Any],
    params: SmardDataParams,
    timestamp: int,
) -> bool:
    """
    Save data to a local JSON file.

    Args:
        data: Data to save
        params: SMARD data parameters
        timestamp: Timestamp

    Returns:
        True on success, False on error

    """
    filename = (
        _config.data_dir
        / f"data_{params.filter_param}_{params.region}_{params.filter_copy}_"
        f"{params.region_copy}_{params.resolution}_{timestamp}.json"
    )
    data_container = {
        "filter": params.filter_param,
        "region": params.region,
        "filter_copy": params.filter_copy,
        "region_copy": params.region_copy,
        "resolution": params.resolution,
        "timestamp": timestamp,
        "data": data,
        "saved_at": datetime.now(UTC).isoformat(),
    }

    try:
        with filename.open("w", encoding="utf-8") as f:
            json.dump(data_container, f, indent=2, ensure_ascii=False)
        _LOGGER.debug("Data saved to: %s", filename)
    except OSError:
        _LOGGER.exception("Error saving data")
        return False
    else:
        return True


def load_data_from_file(
    params: SmardDataParams,
    timestamp: int,
) -> dict[str, Any] | None:
    """
    Load data from a local JSON file.

    Args:
        params: SMARD data parameters
        timestamp: Timestamp

    Returns:
        Data dictionary or None on error

    """
    filename = (
        _config.data_dir
        / f"data_{params.filter_param}_{params.region}_{params.filter_copy}_"
        f"{params.region_copy}_{params.resolution}_{timestamp}.json"
    )

    try:
        with filename.open(encoding="utf-8") as f:
            data_container = json.load(f)
        _LOGGER.debug("Data loaded from: %s", filename)
        _LOGGER.debug("Saved at: %s", data_container.get("saved_at", "Unknown"))
        return data_container.get("data")
    except FileNotFoundError:
        _LOGGER.debug("Local file not found: %s", filename)
        return None
    except (OSError, json.JSONDecodeError):
        _LOGGER.exception("Error loading data")
        return None


def fetch_smard_data(
    params: SmardDataParams,
    timestamp: int,
) -> dict[str, Any] | None:
    """
    Fetch SMARD data either from local storage or network.

    Args:
        params: SMARD data parameters
        timestamp: Timestamp

    Returns:
        Data dictionary or None on error

    """
    # Try local data first if enabled
    if _config.enable_local:
        _LOGGER.debug("Using local data")
        data = load_data_from_file(params, timestamp)
        if data is not None:
            _LOGGER.debug(
                "Successfully loaded! Number of data points: %d",
                len(data.get("series", [])),
            )
            return data
        _LOGGER.debug("No local data found, loading from network...")

    # Load from network
    _LOGGER.debug("Loading data from network")

    url = (
        f"https://www.smard.de/app/chart_data/{params.filter_param}/"
        f"{params.region}/{params.filter_copy}_{params.region_copy}_"
        f"{params.resolution}_{timestamp}.json"
    )

    try:
        _LOGGER.debug("Loading data from: %s", url)

        response = requests.get(url, timeout=DEFAULT_TIMEOUT)
        response.raise_for_status()

        # Parse JSON data
        data = response.json()

        # Save to local file if local mode is enabled
        if _config.enable_local and data:
            save_data_to_file(data, params, timestamp)

        _LOGGER.debug(
            "Successfully loaded! Number of data points: %d",
            len(data.get("series", [])),
        )
    except requests.RequestException:
        _LOGGER.exception("Error loading data")
        return None
    except json.JSONDecodeError:
        _LOGGER.exception("Error parsing JSON data")
        return None
    else:
        return data


def convert_timestamps_to_datetime(timestamps: list[int]) -> list[datetime]:
    """
    Convert Unix timestamps (ms) to datetime objects.

    Args:
        timestamps: List of Unix timestamps in milliseconds

    Returns:
        List of datetime objects

    """
    datetime_list = []
    for ts in timestamps:
        try:
            # Unix timestamp (milliseconds) to datetime
            dt = datetime.fromtimestamp(ts / 1000, tz=UTC)
            datetime_list.append(dt)
        except (ValueError, OSError) as e:
            _LOGGER.warning("Error with timestamp %s: %s", ts, e)

    return datetime_list


def list_local_files() -> None:
    """Display all locally stored JSON files."""
    _LOGGER.info("=== Local data files ===")

    # Timestamp files
    data_path = Path(_config.data_dir)
    timestamp_files = list(data_path.glob("timestamps_*.json"))
    _LOGGER.info("Timestamp files (%d):", len(timestamp_files))
    for file in sorted(timestamp_files):
        filename = file.name
        size = file.stat().st_size
        mtime = datetime.fromtimestamp(file.stat().st_mtime, tz=UTC)
        _LOGGER.info(
            "  %s (%d Bytes, %s)",
            filename,
            size,
            mtime.strftime("%Y-%m-%d %H:%M:%S"),
        )

    # Data files
    data_files = list(data_path.glob("data_*.json"))
    _LOGGER.info("Data files (%d):", len(data_files))
    for file in sorted(data_files):
        filename = file.name
        size = file.stat().st_size
        mtime = datetime.fromtimestamp(file.stat().st_mtime, tz=UTC)
        _LOGGER.info(
            "  %s (%d Bytes, %s)",
            filename,
            size,
            mtime.strftime("%Y-%m-%d %H:%M:%S"),
        )

    if not timestamp_files and not data_files:
        _LOGGER.info("No local files found.")


def clear_local_cache() -> None:
    """Delete all local cache files."""
    data_path = Path(_config.data_dir)
    json_files = list(data_path.glob("*.json"))
    deleted_count = 0

    for file in json_files:
        try:
            file.unlink()
            deleted_count += 1
            _LOGGER.info("Deleted: %s", file.name)
        except OSError:
            _LOGGER.exception("Error deleting %s", file)

    _LOGGER.info("%d files deleted.", deleted_count)


class TimeRange(NamedTuple):
    """Time range configuration."""

    start: datetime
    end: datetime | None


def configure_time_range(
    start_date: date | None = None,
    end_date: date | None = None,
    start_time: str | None = None,
    end_time: str | None = None,
) -> TimeRange:
    """
    Configure the time range for data queries.

    Args:
        start_date: Start date (optional, default: today)
        end_date: End date (optional, default: None for open end)
        start_time: Start time as string "HH:MM" (optional, default: "00:00")
        end_time: End time as string "HH:MM" (optional, default: None)

    Returns:
        TimeRange with start and optional end datetime

    Examples:
        # Today from 00:00 with open end (default):
        configure_time_range()

        # Specific date and time:
        configure_time_range(
            start_date=date(2025, 9, 18),
            end_date=date(2025, 9, 20)
        )

        # Business hours today:
        configure_time_range(start_time="08:00", end_time="18:00")

        # Multiple days:
        configure_time_range(
            start_date=date(2025, 9, 18),
            end_date=date(2025, 9, 20)
        )

    """
    # Use start_date or fallback to today
    actual_start_date = start_date or datetime.now(UTC).date()

    # Parse start time - default 00:00 if None
    actual_start_time = start_time or "00:00"
    try:
        start_time_obj = time.fromisoformat(actual_start_time)
    except ValueError:
        _LOGGER.warning(
            "Invalid start time '%s', using 00:00",
            actual_start_time,
        )
        start_time_obj = time(0, 0)

    # Create start_datetime
    start_datetime = datetime.combine(
        actual_start_date,
        start_time_obj,
        tzinfo=UTC,
    )

    # Create end_datetime
    end_datetime = None
    if end_date:
        if end_time:
            try:
                end_time_obj = time.fromisoformat(end_time)
            except ValueError:
                _LOGGER.warning("Invalid end time '%s', using 23:59", end_time)
                end_time_obj = time(23, 59, 59)
        else:
            # If only end_date but no end_time: use end of day
            end_time_obj = time(23, 59, 59)

        end_datetime = datetime.combine(end_date, end_time_obj, tzinfo=UTC)
    elif end_time:
        # If end_time but no end_date: use start_date as end_date
        try:
            end_time_obj = time.fromisoformat(end_time)
            end_datetime = datetime.combine(
                actual_start_date,
                end_time_obj,
                tzinfo=UTC,
            )
        except ValueError:
            _LOGGER.warning("Invalid end time '%s', using open end", end_time)
            end_datetime = None

    # Debug output
    _LOGGER.info("Time range configured:")
    _LOGGER.info("  Start: %s", start_datetime.strftime("%Y-%m-%d %H:%M:%S"))
    if end_datetime:
        _LOGGER.info("  End:  %s", end_datetime.strftime("%Y-%m-%d %H:%M:%S"))
        days_span = (end_datetime.date() - start_datetime.date()).days + 1
        _LOGGER.info("  Timespan: %d day(s)", days_span)
    else:
        _LOGGER.info("  End:  Open end (all available data)")

    return TimeRange(start=start_datetime, end=end_datetime)


def find_optimal_start_timestamp(
    timestamps: list[int],
    datetime_list: list[datetime],
    start_datetime: datetime,
) -> int:
    """
    Find the optimal start timestamp through backward search.

    Starts from the last (newest) timestamp and goes backward until the first
    timestamp is found that is smaller than the start date.

    Args:
        timestamps: List of available timestamps
        datetime_list: Converted datetime objects of timestamps
        start_datetime: Desired start time

    Returns:
        Index of the optimal start timestamp

    """
    start_date = start_datetime.date()

    _LOGGER.debug("Searching from newest timestamp backward...")
    _LOGGER.debug("Target date: %s", start_date)

    # Start from the last (newest) element and go backward
    for i in range(len(timestamps) - 1, -1, -1):
        timestamp_date = datetime_list[i].date()

        _LOGGER.debug("Index %d: %s", i, timestamp_date)

        # Find the first timestamp that is smaller than the start date
        if timestamp_date < start_date:
            _LOGGER.debug("Optimal start timestamp found!")
            _LOGGER.debug("Index: %d (Timestamp: %d)", i, timestamps[i])
            _LOGGER.debug("Date: %s < %s", timestamp_date, start_date)
            return i

        # Also accept exact match
        if timestamp_date == start_date:
            _LOGGER.debug("Exact match found!")
            _LOGGER.debug("Index: %d (Timestamp: %d)", i, timestamps[i])
            _LOGGER.debug("Date: %s = %s", timestamp_date, start_date)
            return i

    # If no matching timestamp was found
    _LOGGER.debug("No timestamp < %s found", start_date)
    _LOGGER.debug("Using oldest available timestamp (Index 0)")
    return 0


class DataLoadConfig(NamedTuple):
    """Configuration for data loading."""

    filter_param: str = "1223"
    region: str = "DE"
    resolution: str = "quarterhour"


def _process_series_data(
    series: list[Any],
    time_range: TimeRange,
    source_timestamp: int,
) -> list[dict[str, Any]]:
    """
    Process series data and filter by time range.

    Args:
        series: Series data from API
        time_range: Time range to filter
        source_timestamp: Source timestamp ID

    Returns:
        List of data point dictionaries

    """
    data_points = []
    for entry in series:
        if not entry or len(entry) < MIN_SERIES_ENTRY_LENGTH or entry[1] is None:
            continue

        entry_datetime = datetime.fromtimestamp(entry[0] / 1000, tz=UTC)

        # Check if datetime is in range
        in_range = entry_datetime >= time_range.start
        if time_range.end:
            in_range = in_range and entry_datetime <= time_range.end

        if in_range:
            data_points.append(
                {
                    "timestamp_ms": entry[0],
                    "datetime": entry_datetime,
                    "date": entry_datetime.date(),
                    "time": entry_datetime.time(),
                    "value": entry[1],
                    "source_timestamp": source_timestamp,
                }
            )

    return data_points


def _show_data_preview(timestamp_points: list[dict[str, Any]]) -> None:
    """
    Show first and last 5 data entries for debugging.

    Args:
        timestamp_points: List of data points

    """
    _LOGGER.debug("First 5 entries:")
    for j, point in enumerate(timestamp_points[:PREVIEW_ENTRIES_COUNT]):
        _LOGGER.debug(
            "  %d. %s - %.2f €/MWh",
            j + 1,
            point["datetime"].strftime("%Y-%m-%d %H:%M"),
            point["value"],
        )

    if len(timestamp_points) > PREVIEW_ENTRIES_COUNT:
        _LOGGER.debug("Last 5 entries:")
        for j, point in enumerate(timestamp_points[-PREVIEW_ENTRIES_COUNT:]):
            idx = len(timestamp_points) - 4 + j
            _LOGGER.debug(
                "  %d. %s - %.2f €/MWh",
                idx,
                point["datetime"].strftime("%Y-%m-%d %H:%M"),
                point["value"],
            )


def load_data_from_timestamp(
    timestamps: list[int],
    start_index: int,
    time_range: TimeRange,
    config: DataLoadConfig,
) -> tuple[pd.DataFrame, list[int]]:
    """
    Load data from a specific timestamp index.

    Optimized version that starts from the found optimal start timestamp.

    Args:
        timestamps: List of available timestamps
        start_index: Index of the start timestamp
        time_range: Time range for the data
        config: Configuration for data loading

    Returns:
        Tuple of (DataFrame, used_timestamps list)

    """
    all_data_points: list[dict[str, Any]] = []
    used_timestamps: list[int] = []

    _LOGGER.info("Loading data from index %d:", start_index)
    _LOGGER.info("  Start: %s", time_range.start.strftime("%Y-%m-%d %H:%M:%S"))
    if time_range.end:
        _LOGGER.info("  End:  %s", time_range.end.strftime("%Y-%m-%d %H:%M:%S"))
    else:
        _LOGGER.info("  End:  Open end (all available data)")

    params = SmardDataParams(
        filter_param=config.filter_param,
        region=config.region,
        filter_copy=config.filter_param,
        region_copy=config.region,
        resolution=config.resolution,
    )

    # Start at the found start timestamp and go forward
    _LOGGER.debug(
        "Loading data from start timestamp (Index %d forward)",
        start_index,
    )

    for i in range(start_index, len(timestamps)):
        timestamp = timestamps[i]
        timestamp_dt = datetime.fromtimestamp(timestamp / 1000, tz=UTC)

        _LOGGER.debug(
            "Checking timestamp %d (%s)...",
            timestamp,
            timestamp_dt.strftime("%Y-%m-%d"),
        )

        try:
            # Load data for this timestamp
            data = fetch_smard_data(params, timestamp)

            if not data or "series" not in data:
                _LOGGER.debug("  No data available")
                continue

            # Collect data in the specified time range
            timestamp_points = _process_series_data(
                data["series"],
                time_range,
                timestamp,
            )

            # Save the data
            if timestamp_points:
                # Avoid duplicates
                existing_timestamps = {dp["timestamp_ms"] for dp in all_data_points}
                new_points = [
                    dp
                    for dp in timestamp_points
                    if dp["timestamp_ms"] not in existing_timestamps
                ]

                if new_points:
                    all_data_points.extend(new_points)
                    used_timestamps.append(timestamp)
                    _LOGGER.debug(
                        "  %d new data points in time range loaded",
                        len(new_points),
                    )

                    # Debug: Show first and last 5 entries
                    _show_data_preview(new_points)

                    # Check if end date is reached
                    if time_range.end:
                        latest_date = max(dp["date"] for dp in new_points)
                        if latest_date >= time_range.end.date():
                            _LOGGER.debug(
                                "  End date %s reached!",
                                time_range.end.date(),
                            )
                            break
                else:
                    _LOGGER.debug("  Data already present (duplicates)")
            else:
                _LOGGER.debug("  No data in time range")

        except (requests.RequestException, json.JSONDecodeError) as e:
            _LOGGER.warning("  Error: %s", e)
            continue

    # Convert to pandas DataFrame
    if all_data_points:
        df = pd.DataFrame(all_data_points)
        _LOGGER.info("=== SUMMARY ===")
        _LOGGER.info("DataFrame created: %d data points", len(df))
        _LOGGER.info("Used timestamps: %d", len(used_timestamps))
        _LOGGER.info("DataFrame columns: %s", list(df.columns))
        return df, used_timestamps

    _LOGGER.warning("No data found - returning empty DataFrame")
    return pd.DataFrame(), used_timestamps


def analyze_data(
    df: pd.DataFrame,
    target_date: date,
    used_timestamps: list[int],
) -> None:
    """
    Analyze the loaded data.

    Args:
        df: DataFrame with loaded data
        target_date: Target date for analysis
        used_timestamps: List of used timestamp IDs

    """
    _LOGGER.info("=== DATA ANALYSIS ===")
    _LOGGER.info("Timestamps used: %d", len(used_timestamps))
    _LOGGER.info("Data points: %d", len(df))

    if len(df) > 0:
        start_time = df["datetime"].min()
        end_time = df["datetime"].max()
        _LOGGER.info(
            "Time range: %s to %s",
            start_time.strftime("%Y-%m-%d %H:%M"),
            end_time.strftime("%Y-%m-%d %H:%M"),
        )

        # Price statistics
        _LOGGER.info("Price statistics:")
        _LOGGER.info("   Min: %.2f €/MWh", df["value"].min())
        _LOGGER.info("   Max: %.2f €/MWh", df["value"].max())
        _LOGGER.info("   Average: %.2f €/MWh", df["value"].mean())
        _LOGGER.info("   Median: %.2f €/MWh", df["value"].median())

        # Daily analysis
        unique_dates = sorted(df["date"].unique())
        _LOGGER.info("Covered days: %d", len(unique_dates))
        for date_val in unique_dates:
            count = len(df[df["date"] == date_val])
            marker = "Target" if date_val == target_date else "Day"
            _LOGGER.info("   %s %s: %d data points", marker, date_val, count)


def export_csv(df: pd.DataFrame, data_dir: Path) -> None:
    """
    Export data as CSV.

    Args:
        df: DataFrame to export
        data_dir: Directory for CSV file

    """
    if len(df) == 0:
        _LOGGER.error("No data to export")
        return

    # Prepare export DataFrame
    export_df = df[["datetime", "date", "time", "value"]].copy()
    export_df.columns = ["Timestamp", "Date", "Time", "Price_EUR_MWh"]

    # Determine filename
    start_date = df["date"].min()
    end_date = df["date"].max()

    if start_date == end_date:
        filename = f"smard_data_{start_date.strftime('%Y%m%d')}.csv"
    else:
        filename = (
            f"smard_data_{start_date.strftime('%Y%m%d')}_"
            f"to_{end_date.strftime('%Y%m%d')}.csv"
        )

    filepath = data_dir / filename

    # Ensure directory exists
    data_dir.mkdir(parents=True, exist_ok=True)

    # Export
    export_df.to_csv(filepath, index=False, encoding="utf-8")

    _LOGGER.info("=== CSV EXPORT ===")
    _LOGGER.info("File: %s", filepath)
    _LOGGER.info("Data points: %d", len(export_df))
    _LOGGER.info("Time range: %s to %s", start_date, end_date)
    _LOGGER.info("File size: %d Bytes", filepath.stat().st_size)


def convert_euro_mwh_to_ct_kwh(value_euro_mwh: float | None) -> float | None:
    """
    Convert energy prices from Euro/MWh to Cent/kWh.

    Args:
        value_euro_mwh: Price in Euro/MWh (float or int)

    Returns:
        Price in Cent/kWh or None if input is None

    Example:
        >>> convert_euro_mwh_to_ct_kwh(120.0)  # 120 Euro/MWh
        12.0  # = 12 Cent/kWh

    Conversion formula:
        1 Euro/MWh = 0.1 Cent/kWh
        (because 1 MWh = 1000 kWh and 1 Euro = 100 Cent)

    """
    if value_euro_mwh is None:
        return None

    try:
        # 1 Euro/MWh = 0.1 Cent/kWh
        value_ct_kwh = float(value_euro_mwh) * 0.1
        return round(value_ct_kwh, 4)  # 4 decimal places for precision
    except (ValueError, TypeError):
        _LOGGER.warning("Invalid value for conversion: %s", value_euro_mwh)
        return None


def get_all_data_values(df: pd.DataFrame, column_name: str = "value") -> list[Any]:
    """
    Extract all values from a pandas DataFrame column.

    Args:
        df: pandas DataFrame with the data
        column_name: Name of the column to extract values from (default: 'value')

    Returns:
        List of all values (without None/NaN values)

    Example:
        >>> df = pd.DataFrame({'value': [120.5, None, 130.2], 'other': [1, 2, 3]})
        >>> get_all_data_values(df, 'value')
        [120.5, 130.2]

    """
    if df.empty:
        _LOGGER.warning("DataFrame is empty")
        return []

    if column_name not in df.columns:
        _LOGGER.warning("Column '%s' not found in DataFrame", column_name)
        _LOGGER.warning("Available columns: %s", list(df.columns))
        return []

    # Remove None/NaN values and convert to list
    values = df[column_name].dropna().tolist()

    _LOGGER.info(
        "%d values extracted from column '%s' (%d total rows)",
        len(values),
        column_name,
        len(df),
    )
    return values
